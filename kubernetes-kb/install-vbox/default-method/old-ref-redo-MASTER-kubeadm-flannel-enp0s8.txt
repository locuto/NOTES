=============================================================================
REDO MASTER KUBEADM INIT AND FLANNEL INSTALL - with correct interface enp0s8
=============================================================================
With flannel, centos (being used for round 2 POC)
  https://www.techrepublic.com/article/how-to-install-a-kubernetes-cluster-on-centos-7/
  Use it along with https://linuxthegreat.wordpress.com/2017/10/17/installing-kubernetes-1-8-1-on-centos-7/
(to know about kublet not starting)
  https://www.howtoforge.com/tutorial/centos-kubernetes-docker-cluster/
  
  Also refer to https://icicimov.github.io/blog/kubernetes/Kubernetes-cluster-step-by-step-Part4/
  (just that this one shows no cni0 in nodes - only in master)
  
=================
MASTER
=================

------------------
INSTALL SOFTWARE
------------------
NOTE: This part is common to master and nodes

Install docker
Install kubelet, kubeadm, ....

# mkdir /root/kubeinstall
# cd /root/kubeinstall

Ensure '1' in the following:
# echo '1' > /proc/sys/net/bridge/bridge-nf-call-iptables
# cat /proc/sys/net/bridge/bridge-nf-call-iptables
1

# systemctl enable docker
# systemctl enable kubelet
# service docker start

------------------------
KUBEADM INIT
------------------------
NOTE: Master only

# kubeadm config images pull 
--> Optional - if you want to later disable NAT and run kubeadm.  This will pull images for kubeadm beforehand
[config/images] Pulled k8s.gcr.io/kube-apiserver:v1.13.1
[config/images] Pulled k8s.gcr.io/kube-controller-manager:v1.13.1
[config/images] Pulled k8s.gcr.io/kube-scheduler:v1.13.1
[config/images] Pulled k8s.gcr.io/kube-proxy:v1.13.1
[config/images] Pulled k8s.gcr.io/pause:3.1
[config/images] Pulled k8s.gcr.io/etcd:3.2.24
[config/images] Pulled k8s.gcr.io/coredns:1.2.6


Check routes before kubeadm
# route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         10.0.2.2        0.0.0.0         UG    100    0        0 enp0s3
10.0.2.0        0.0.0.0         255.255.255.0   U     100    0        0 enp0s3
172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0
192.168.11.0    0.0.0.0         255.255.255.0   U     101    0        0 enp0s8
192.168.12.0    0.0.0.0         255.255.255.0   U     102    0        0 enp0s9
192.168.122.0   0.0.0.0         255.255.255.0   U     0      0        0 virbr0

Run kubeadm init:
root@k00 ~]# kubeadm init --apiserver-advertise-address=192.168.11.10 --pod-network-cidr=192.168.0.0/16 --ignore-preflight-errors=all
[init] using Kubernetes version: v1.12.3
[preflight] running pre-flight checks
	[WARNING FileContent--proc-sys-net-bridge-bridge-nf-call-iptables]: /proc/sys/net/bridge/bridge-nf-call-iptables contents are not set to 1
[preflight] The system verification failed. Printing the output from the verification:
KERNEL_VERSION: 4.1.12-112.16.4.el7uek.x86_64
CONFIG_NAMESPACES: enabled
CONFIG_NET_NS: enabled
CONFIG_PID_NS: enabled
CONFIG_IPC_NS: enabled
CONFIG_UTS_NS: enabled
CONFIG_CGROUPS: enabled
CONFIG_CGROUP_CPUACCT: enabled
CONFIG_CGROUP_DEVICE: enabled
CONFIG_CGROUP_FREEZER: enabled
CONFIG_CGROUP_SCHED: enabled
CONFIG_CPUSETS: enabled
CONFIG_MEMCG: enabled
CONFIG_INET: enabled
CONFIG_EXT4_FS: enabled (as module)
CONFIG_PROC_FS: enabled
CONFIG_NETFILTER_XT_TARGET_REDIRECT: enabled (as module)
CONFIG_NETFILTER_XT_MATCH_COMMENT: enabled (as module)
CONFIG_OVERLAY_FS: enabled (as module)
CONFIG_AUFS_FS: not set - Required for aufs.
CONFIG_BLK_DEV_DM: enabled (as module)
DOCKER_VERSION: 18.09.0
OS: Linux
CGROUPS_CPU: enabled
CGROUPS_CPUACCT: enabled
CGROUPS_CPUSET: enabled
CGROUPS_DEVICES: enabled
CGROUPS_FREEZER: enabled
CGROUPS_MEMORY: enabled
	[WARNING SystemVerification]: unsupported docker version: 18.09.0
	[WARNING Hostname]: hostname "k00" could not be reached
	[WARNING Hostname]: hostname "k00" lookup k00 on 10.97.40.216:53: no such host
[preflight/images] Pulling images required for setting up a Kubernetes cluster
[preflight/images] This might take a minute or two, depending on the speed of your internet connection
[preflight/images] You can also perform this action in beforehand using 'kubeadm config images pull'
[kubelet] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[preflight] Activating the kubelet service
[certificates] Generated front-proxy-ca certificate and key.
[certificates] Generated front-proxy-client certificate and key.
[certificates] Generated etcd/ca certificate and key.
[certificates] Generated etcd/healthcheck-client certificate and key.
[certificates] Generated apiserver-etcd-client certificate and key.
[certificates] Generated etcd/server certificate and key.
[certificates] etcd/server serving cert is signed for DNS names [k00 localhost] and IPs [127.0.0.1 ::1]
[certificates] Generated etcd/peer certificate and key.
[certificates] etcd/peer serving cert is signed for DNS names [k00 localhost] and IPs [192.168.11.10 127.0.0.1 ::1]
[certificates] Generated ca certificate and key.
[certificates] Generated apiserver certificate and key.
[certificates] apiserver serving cert is signed for DNS names [k00 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.11.10]
[certificates] Generated apiserver-kubelet-client certificate and key.
[certificates] valid certificates and keys now exist in "/etc/kubernetes/pki"
[certificates] Generated sa key and public key.
[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/admin.conf"
[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/kubelet.conf"
[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/controller-manager.conf"
[kubeconfig] Wrote KubeConfig file to disk: "/etc/kubernetes/scheduler.conf"
[controlplane] wrote Static Pod manifest for component kube-apiserver to "/etc/kubernetes/manifests/kube-apiserver.yaml"
[controlplane] wrote Static Pod manifest for component kube-controller-manager to "/etc/kubernetes/manifests/kube-controller-manager.yaml"
[controlplane] wrote Static Pod manifest for component kube-scheduler to "/etc/kubernetes/manifests/kube-scheduler.yaml"
[etcd] Wrote Static Pod manifest for a local etcd instance to "/etc/kubernetes/manifests/etcd.yaml"
[init] waiting for the kubelet to boot up the control plane as Static Pods from directory "/etc/kubernetes/manifests" 
[init] this might take a minute or longer if the control plane images have to be pulled
[apiclient] All control plane components are healthy after 33.003144 seconds
[uploadconfig] storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.12" in namespace kube-system with the configuration for the kubelets in the cluster
[markmaster] Marking the node k00 as master by adding the label "node-role.kubernetes.io/master=''"
[markmaster] Marking the node k00 as master by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[patchnode] Uploading the CRI Socket information "/var/run/dockershim.sock" to the Node API object "k00" as an annotation
[bootstraptoken] using token: cysre4.dqlfk8ys8d8p9dyq
[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstraptoken] creating the "cluster-info" ConfigMap in the "kube-public" namespace
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes master has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join 192.168.11.10:6443 --token cysre4.dqlfk8ys8d8p9dyq --discovery-token-ca-cert-hash sha256:b4352f0c6f0553fefb7656b9000450f3d6e6bceac715dfbfdbccf2a3c2de69d4

Check routes:
# route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         10.0.2.2        0.0.0.0         UG    100    0        0 enp0s3
10.0.2.0        0.0.0.0         255.255.255.0   U     100    0        0 enp0s3
172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0
192.168.11.0    0.0.0.0         255.255.255.0   U     101    0        0 enp0s8
192.168.12.0    0.0.0.0         255.255.255.0   U     102    0        0 enp0s9
192.168.122.0   0.0.0.0         255.255.255.0   U     0      0        0 virbr0

# ip route
default via 10.0.2.2 dev enp0s3 proto dhcp metric 100 
10.0.2.0/24 dev enp0s3 proto kernel scope link src 10.0.2.15 metric 100 
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 
192.168.11.0/24 dev enp0s8 proto kernel scope link src 192.168.11.10 metric 101 
192.168.12.0/24 dev enp0s9 proto kernel scope link src 192.168.12.10 metric 102 
192.168.122.0/24 dev virbr0 proto kernel scope link src 192.168.122.1 

Check IPs: 
No change. Just the machine and docker interfaces.

# ip addr list
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default 
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
2: enp0s3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:10:44:20 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global noprefixroute dynamic enp0s3
       valid_lft 80419sec preferred_lft 80419sec
    inet6 fe80::2481:bef7:e0d6:5810/64 scope link noprefixroute 
       valid_lft forever preferred_lft forever

3: enp0s8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:7b:13:15 brd ff:ff:ff:ff:ff:ff
    inet 192.168.11.10/24 brd 192.168.11.255 scope global noprefixroute enp0s8
       valid_lft forever preferred_lft forever
    inet6 fe80::5270:6f3b:6966:ad14/64 scope link noprefixroute 
       valid_lft forever preferred_lft forever
4: enp0s9: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:ca:34:38 brd ff:ff:ff:ff:ff:ff
    inet 192.168.12.10/24 brd 192.168.12.255 scope global noprefixroute enp0s9
       valid_lft forever preferred_lft forever
    inet6 fe80::b331:1d54:d60b:62c1/64 scope link noprefixroute 
       valid_lft forever preferred_lft forever
5: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default 
    link/ether 02:42:f5:8d:0a:0f brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
6: virbr0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default 
    link/ether 52:54:00:cc:ab:31 brd ff:ff:ff:ff:ff:ff
    inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0
       valid_lft forever preferred_lft forever
7: virbr0-nic: <BROADCAST,MULTICAST> mtu 1500 qdisc pfifo_fast master virbr0 state DOWN group default qlen 500
    link/ether 52:54:00:cc:ab:31 brd ff:ff:ff:ff:ff:ff


------------------------
INSTALL FLANNEL
------------------------
NOTE: THIS IS COMMON TO MASTER AND NODE

Pass the correct node-ip to kubelet (so that it wont take the NAT IP)

# ps -ef |grep kubelet
root     17390     1 16 12:46 ?        00:00:00 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --cgroup-driver=cgroupfs --network-plugin=cni

- Edit the file /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
Modify the Environment line --> add node-ip:
Environment="KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.11.10"

- Restart kubelet
# systemctl daemon-reload
# service kubelet restart

# ps -ef |grep kublelet
root     17390     1  2 12:46 ?        00:00:03 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.11.10 --config=/var/lib/kubelet/config.yaml --cgroup-driver=cgroupfs --network-plugin=cni

(now you see kubelet has node-ip parameter also)

- INSTALL FLANNEL

Ensure that this shows '1'
# cat /proc/sys/net/bridge/bridge-nf-call-iptables
1

Create .kube and config file in it
# mkdir -p /root/.kube
# cp /etc/kubernetes/admin.conf /root/.kube/config

Install flannel

# wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

- Edit kube-flannel.yml and add the following 'iface' line 
NOTE:  SEARCH FOR subnet-mgr which is there in multiple places - and add iface line below them

      - name: kube-flannel
        image: quay.io/coreos/flannel:v0.10.0-amd64
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        - --iface=enp0s8
	
# kubectl apply -f /root/kubeinstall/kube-flannel.yml

Check IPs
# ip addr list

NOTE: We see flannel1.1 and cni0

[root@k00 kubeinstall]# ip addr list
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default 
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
2: enp0s3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:10:44:20 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global noprefixroute dynamic enp0s3
       valid_lft 85463sec preferred_lft 85463sec
    inet6 fe80::2481:bef7:e0d6:5810/64 scope link noprefixroute 
       valid_lft forever preferred_lft forever
3: enp0s8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:7b:13:15 brd ff:ff:ff:ff:ff:ff
    inet 192.168.11.100/24 brd 192.168.11.255 scope global noprefixroute enp0s8
       valid_lft forever preferred_lft forever
    inet6 fe80::5270:6f3b:6966:ad14/64 scope link noprefixroute 
       valid_lft forever preferred_lft forever
4: enp0s9: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 08:00:27:ca:34:38 brd ff:ff:ff:ff:ff:ff
    inet 192.168.12.100/24 brd 192.168.12.255 scope global noprefixroute enp0s9
       valid_lft forever preferred_lft forever
    inet6 fe80::b331:1d54:d60b:62c1/64 scope link noprefixroute 
       valid_lft forever preferred_lft forever
5: virbr0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default 
    link/ether 52:54:00:cc:ab:31 brd ff:ff:ff:ff:ff:ff
    inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr0
       valid_lft forever preferred_lft forever
6: virbr0-nic: <BROADCAST,MULTICAST> mtu 1500 qdisc pfifo_fast master virbr0 state DOWN group default qlen 500
    link/ether 52:54:00:cc:ab:31 brd ff:ff:ff:ff:ff:ff
7: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default 
    link/ether 02:42:f3:f2:a2:ba brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
8: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UNKNOWN group default 
    link/ether fe:42:f5:00:b0:e1 brd ff:ff:ff:ff:ff:ff
    inet 192.168.0.0/32 scope global flannel.1
       valid_lft forever preferred_lft forever
    inet6 fe80::fc42:f5ff:fe00:b0e1/64 scope link 
       valid_lft forever preferred_lft forever
9: cni0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP group default 
    link/ether 0a:58:c0:a8:00:01 brd ff:ff:ff:ff:ff:ff
    inet 192.168.0.1/24 scope global cni0
       valid_lft forever preferred_lft forever
    inet6 fe80::ab:b6ff:febb:58b5/64 scope link 
       valid_lft forever preferred_lft forever
10: veth8a6fba63@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master cni0 state UP group default 
    link/ether c2:21:47:83:81:46 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet6 fe80::c021:47ff:fe83:8146/64 scope link 
       valid_lft forever preferred_lft forever
11: veth00c589e0@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master cni0 state UP group default 
    link/ether b6:18:a5:a7:66:71 brd ff:ff:ff:ff:ff:ff link-netnsid 1
    inet6 fe80::b418:a5ff:fea7:6671/64 scope link 
       valid_lft forever preferred_lft forever


Check routes:
NOTE: Now we see cni0 properly

[root@k00 kubeinstall]# route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         10.0.2.2        0.0.0.0         UG    100    0        0 enp0s3
10.0.2.0        0.0.0.0         255.255.255.0   U     100    0        0 enp0s3
172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0
192.168.0.0     0.0.0.0         255.255.255.0   U     0      0        0 cni0
192.168.11.0    0.0.0.0         255.255.255.0   U     101    0        0 enp0s8
192.168.12.0    0.0.0.0         255.255.255.0   U     102    0        0 enp0s9
192.168.122.0   0.0.0.0         255.255.255.0   U     0      0        0 virbr0

[root@k00 kubeinstall]# ip r
default via 10.0.2.2 dev enp0s3 proto dhcp metric 100 
10.0.2.0/24 dev enp0s3 proto kernel scope link src 10.0.2.15 metric 100 
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 
192.168.0.0/24 dev cni0 proto kernel scope link src 192.168.0.1 
192.168.11.0/24 dev enp0s8 proto kernel scope link src 192.168.11.100 metric 101 
192.168.12.0/24 dev enp0s9 proto kernel scope link src 192.168.12.100 metric 102 
192.168.122.0/24 dev virbr0 proto kernel scope link src 192.168.122.1 

============================
CHECK A FEW MORE THINGS
============================
# kubectl cluster-info
Kubernetes master is running at https://192.168.11.100:6443
KubeDNS is running at https://192.168.11.100:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

